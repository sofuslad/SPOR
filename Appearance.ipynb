{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6466762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 1491\n",
      "Data validation passed!\n",
      "Train dataset size: 1043\n",
      "Test dataset size: 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "<positron-console-cell-1>:53: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "Map: 100%|██████████| 1043/1043 [00:00<00:00, 42948.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 25032\n",
      "  Batch size = 24\n",
      "  Num epochs = 1\n",
      "  0%|          | 0/1043 [00:00<?, ?it/s]/Users/johanheinsen/.pyenv/versions/3.11.12/envs/my-3.11-environment/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  0%|          | 1/1043 [00:02<43:40,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.255, 'grad_norm': 2.649540662765503, 'learning_rate': 0.0, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1043 [00:21<27:07,  1.58s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"efterlysninger_1500.csv\")\n",
    "\n",
    "# Clean label column\n",
    "df[\"Signalement?\"] = pd.to_numeric(df[\"Signalement?\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"label\"] = df[\"Signalement?\"]\n",
    "\n",
    "# Clean text column - simpler approach\n",
    "df = df.dropna(subset=['Text'])\n",
    "df['text'] = df['Text'].astype(str).str.strip()\n",
    "df = df[df['text'] != ''].copy()\n",
    "\n",
    "print(f\"Dataset size after cleaning: {len(df)}\")\n",
    "\n",
    "# Validation\n",
    "assert not df['text'].isna().any(), \"Still have missing text values\"\n",
    "assert not df['label'].isna().any(), \"Still have missing label values\"\n",
    "print(\"Data validation passed!\")\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=49)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Load model\n",
    "model = SetFitModel.from_pretrained(\"JohanHeinsen/Old_News_Segmentation_SBERT_V0.1\")\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(preds, labels):\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    return {\n",
    "        \"accuracy\": acc, \n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Train\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    metric=compute_metrics,\n",
    "    batch_size=24,\n",
    "    num_iterations=12, #44\n",
    "    num_epochs=1,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
