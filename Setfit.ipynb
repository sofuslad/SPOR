{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8aa376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after cleaning: 1500\n",
      "Data validation passed!\n",
      "Train dataset size: 1050\n",
      "Test dataset size: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "<positron-console-cell-4>:53: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "Map: 100%|██████████| 1050/1050 [00:00<00:00, 29411.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 92400\n",
      "  Batch size = 24\n",
      "  Num epochs = 1\n",
      "  0%|          | 0/3850 [00:00<?, ?it/s]/Users/johanheinsen/.pyenv/versions/3.11.12/envs/my-3.11-environment/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  0%|          | 1/3850 [00:01<1:42:56,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2644, 'grad_norm': 2.91963791847229, 'learning_rate': 0.0, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 50/3850 [01:50<2:04:56,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2902, 'grad_norm': 3.143937110900879, 'learning_rate': 2.5454545454545456e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3850 [03:30<1:49:42,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1297, 'grad_norm': 1.4747860431671143, 'learning_rate': 5.142857142857142e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 150/3850 [04:57<1:52:58,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0355, 'grad_norm': 0.30196502804756165, 'learning_rate': 7.74025974025974e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 200/3850 [06:30<2:06:36,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.02, 'grad_norm': 0.8009745478630066, 'learning_rate': 1.0337662337662338e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 250/3850 [08:02<1:47:16,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0086, 'grad_norm': 0.4462618827819824, 'learning_rate': 1.2935064935064937e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 300/3850 [09:41<1:53:06,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0039, 'grad_norm': 0.03445606306195259, 'learning_rate': 1.5532467532467534e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 350/3850 [11:06<1:38:41,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0024, 'grad_norm': 0.018141375854611397, 'learning_rate': 1.812987012987013e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 400/3850 [12:43<2:01:41,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0019, 'grad_norm': 0.03088948503136635, 'learning_rate': 1.991919191919192e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 450/3850 [14:14<1:39:17,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0007, 'grad_norm': 0.02756504714488983, 'learning_rate': 1.963059163059163e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 461/3850 [14:33<1:38:24,  1.74s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"efterlysninger_1500.csv\")\n",
    "\n",
    "# Clean label column\n",
    "df[\"Kvinde?\"] = pd.to_numeric(df[\"Kvinde?\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"label\"] = df[\"Kvinde?\"]\n",
    "\n",
    "# Clean text column - simpler approach\n",
    "df = df.dropna(subset=['Text'])\n",
    "df['text'] = df['Text'].astype(str).str.strip()\n",
    "df = df[df['text'] != ''].copy()\n",
    "\n",
    "print(f\"Dataset size after cleaning: {len(df)}\")\n",
    "\n",
    "# Validation\n",
    "assert not df['text'].isna().any(), \"Still have missing text values\"\n",
    "assert not df['label'].isna().any(), \"Still have missing label values\"\n",
    "print(\"Data validation passed!\")\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=49)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Load model\n",
    "model = SetFitModel.from_pretrained(\"JohanHeinsen/Old_News_Segmentation_SBERT_V0.1\")\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(preds, labels):\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "    return {\n",
    "        \"accuracy\": acc, \n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Train\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    metric=compute_metrics,\n",
    "    batch_size=24,\n",
    "    num_iterations=44,\n",
    "    num_epochs=1,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
